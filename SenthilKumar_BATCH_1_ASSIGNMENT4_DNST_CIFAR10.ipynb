{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10180,
     "status": "ok",
     "timestamp": 1519125318845,
     "user": {
      "displayName": "Rohan Shravan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116259693572096743881"
     },
     "user_tz": -330
    },
    "id": "K70hAckqg0EA",
    "outputId": "2fc6e1dd-b97c-46c9-9b74-1738093a5255"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssk/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/\n",
    "!pip install -q keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# batch_size = 128\n",
    "batch_size = 16\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "l = 40\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11693,
     "status": "ok",
     "timestamp": 1519125334438,
     "user": {
      "displayName": "Rohan Shravan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116259693572096743881"
     },
     "user_tz": -330
    },
    "id": "mB7o3zu1g6eT",
    "outputId": "c1cea922-a38d-45da-f9d8-7977ab9c2dd2"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OOP6IPsGhBwb"
   },
   "outputs": [],
   "source": [
    "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0RaKFpubhDIC"
   },
   "outputs": [],
   "source": [
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "num_filter = 12\n",
    "dropout_rate = 0.2\n",
    "l = 12\n",
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 9860
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 698,
     "status": "ok",
     "timestamp": 1519125345471,
     "user": {
      "displayName": "Rohan Shravan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116259693572096743881"
     },
     "user_tz": -330
    },
    "id": "1kFh7pdxhNtT",
    "outputId": "160abc05-9e09-4454-d453-0e33a7d95796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 12)   324         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 12)   48          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 12)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 6)    648         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 6)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 18)   0           conv2d_1[0][0]                   \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 18)   72          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 18)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 6)    972         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 6)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 24)   0           concatenate_1[0][0]              \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 24)   96          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 24)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 6)    1296        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 6)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 30)   0           concatenate_2[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 30)   120         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 30)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 6)    1620        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 6)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 36)   0           concatenate_3[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 36)   144         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 36)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 6)    1944        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 6)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 42)   0           concatenate_4[0][0]              \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 42)   168         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 42)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 6)    2268        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 6)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 48)   0           concatenate_5[0][0]              \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 48)   192         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 6)    2592        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 6)    0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 54)   0           concatenate_6[0][0]              \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 54)   216         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 54)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 6)    2916        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 32, 6)    0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 60)   0           concatenate_7[0][0]              \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 60)   240         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 60)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 6)    3240        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32, 32, 6)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 66)   0           concatenate_8[0][0]              \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 66)   264         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 66)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 6)    3564        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 32, 32, 6)    0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 72)   0           concatenate_9[0][0]              \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 72)   288         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 72)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 6)    3888        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32, 32, 6)    0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 78)   0           concatenate_10[0][0]             \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 78)   312         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 78)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 6)    4212        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32, 32, 6)    0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 84)   0           concatenate_11[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 84)   336         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 84)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 6)    504         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 32, 32, 6)    0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 6)    0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 6)    24          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 6)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 6)    324         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 16, 16, 6)    0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 12)   0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 12)   48          concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 12)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 6)    648         activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 16, 16, 6)    0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 16, 16, 18)   0           concatenate_13[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 18)   72          concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 18)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 6)    972         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16, 16, 6)    0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 24)   0           concatenate_14[0][0]             \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 24)   96          concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 24)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 6)    1296        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16, 16, 6)    0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 30)   0           concatenate_15[0][0]             \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 30)   120         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 30)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 6)    1620        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16, 16, 6)    0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 36)   0           concatenate_16[0][0]             \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 36)   144         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 36)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 6)    1944        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 16, 16, 6)    0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 42)   0           concatenate_17[0][0]             \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 42)   168         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 42)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 6)    2268        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 16, 16, 6)    0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 48)   0           concatenate_18[0][0]             \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 48)   192         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 6)    2592        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 16, 16, 6)    0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 54)   0           concatenate_19[0][0]             \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 54)   216         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 54)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 6)    2916        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 16, 16, 6)    0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 16, 16, 60)   0           concatenate_20[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 60)   240         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 60)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 6)    3240        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 16, 16, 6)    0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 16, 16, 66)   0           concatenate_21[0][0]             \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 66)   264         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 66)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 6)    3564        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 16, 16, 6)    0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 16, 16, 72)   0           concatenate_22[0][0]             \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 72)   288         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 72)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 6)    3888        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 16, 16, 6)    0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 16, 16, 78)   0           concatenate_23[0][0]             \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 78)   312         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 78)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 6)    468         activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 16, 16, 6)    0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 8, 8, 6)      0           dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 6)      24          average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 6)      0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 6)      324         activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 8, 8, 6)      0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 12)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 12)     48          concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 12)     0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 6)      648         activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 8, 8, 6)      0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 8, 8, 18)     0           concatenate_25[0][0]             \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 18)     72          concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 18)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 6)      972         activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 8, 8, 6)      0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 8, 8, 24)     0           concatenate_26[0][0]             \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 24)     96          concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 24)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 6)      1296        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 8, 8, 6)      0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 8, 8, 30)     0           concatenate_27[0][0]             \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 30)     120         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 30)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 6)      1620        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 8, 8, 6)      0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 8, 8, 36)     0           concatenate_28[0][0]             \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 36)     144         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 36)     0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 6)      1944        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 8, 8, 6)      0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 8, 8, 42)     0           concatenate_29[0][0]             \n",
      "                                                                 dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 8, 42)     168         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 42)     0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 6)      2268        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 8, 8, 6)      0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 8, 8, 48)     0           concatenate_30[0][0]             \n",
      "                                                                 dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 48)     192         concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 48)     0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 6)      2592        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 8, 8, 6)      0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 8, 8, 54)     0           concatenate_31[0][0]             \n",
      "                                                                 dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 54)     216         concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 54)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 6)      2916        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 8, 8, 6)      0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 60)     0           concatenate_32[0][0]             \n",
      "                                                                 dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 60)     240         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 60)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 6)      3240        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 8, 8, 6)      0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 66)     0           concatenate_33[0][0]             \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 66)     264         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 66)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 6)      3564        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 8, 8, 6)      0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 72)     0           concatenate_34[0][0]             \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 72)     288         concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 72)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 6)      3888        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 8, 8, 6)      0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 8, 8, 78)     0           concatenate_35[0][0]             \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 78)     312         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 78)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 6)      468         activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 8, 8, 6)      0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 4, 4, 6)      0           dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 4, 4, 6)      24          average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 4, 4, 6)      0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 4, 4, 6)      324         activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 4, 4, 6)      0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 4, 4, 12)     0           average_pooling2d_3[0][0]        \n",
      "                                                                 dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 4, 4, 12)     48          concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 12)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 4, 4, 6)      648         activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 4, 4, 6)      0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 4, 4, 18)     0           concatenate_37[0][0]             \n",
      "                                                                 dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 4, 4, 18)     72          concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 18)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 4, 4, 6)      972         activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 4, 4, 6)      0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 4, 4, 24)     0           concatenate_38[0][0]             \n",
      "                                                                 dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 4, 4, 24)     96          concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 24)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 4, 4, 6)      1296        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 4, 4, 6)      0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 4, 4, 30)     0           concatenate_39[0][0]             \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 4, 4, 30)     120         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 30)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 4, 6)      1620        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 4, 4, 6)      0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 4, 4, 36)     0           concatenate_40[0][0]             \n",
      "                                                                 dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 4, 4, 36)     144         concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 36)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 6)      1944        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 4, 4, 6)      0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 4, 4, 42)     0           concatenate_41[0][0]             \n",
      "                                                                 dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 4, 4, 42)     168         concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 42)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 6)      2268        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 4, 4, 6)      0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 4, 4, 48)     0           concatenate_42[0][0]             \n",
      "                                                                 dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 4, 4, 48)     192         concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 48)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 6)      2592        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 4, 4, 6)      0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 4, 4, 54)     0           concatenate_43[0][0]             \n",
      "                                                                 dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 54)     216         concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 54)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 6)      2916        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 4, 4, 6)      0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 4, 4, 60)     0           concatenate_44[0][0]             \n",
      "                                                                 dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 60)     240         concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 60)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 4, 6)      3240        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 4, 4, 6)      0           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 4, 4, 66)     0           concatenate_45[0][0]             \n",
      "                                                                 dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 4, 66)     264         concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 4, 4, 66)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 4, 4, 6)      3564        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 4, 4, 6)      0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 4, 4, 72)     0           concatenate_46[0][0]             \n",
      "                                                                 dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 4, 72)     288         concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 72)     0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 4, 4, 6)      3888        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 4, 4, 6)      0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 4, 4, 78)     0           concatenate_47[0][0]             \n",
      "                                                                 dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 4, 4, 78)     312         concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 4, 4, 78)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 2, 2, 78)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 312)          0           average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           3130        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 118,918\n",
      "Trainable params: 114,394\n",
      "Non-trainable params: 4,524\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'],\n",
    "             options = run_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1771
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7709652,
     "status": "ok",
     "timestamp": 1519133056963,
     "user": {
      "displayName": "Rohan Shravan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116259693572096743881"
     },
     "user_tz": -330
    },
    "id": "crhGk7kEhXAz",
    "outputId": "e3e2d0d0-1492-41ab-df5b-5a7ecd705c2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[16,54,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: batch_normalization_8/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concatenate_7/concat, batch_normalization_8/gamma/read, batch_normalization_8/beta/read, batch_normalization_1/Const_4, batch_normalization_1/Const_4)]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  3.62MiB from batch_normalization_5/FusedBatchNorm\n  3.38MiB from concatenate_7/concat\n  3.00MiB from batch_normalization_7/FusedBatchNorm\n  3.00MiB from concatenate_6/concat\n  3.00MiB from training_1/Adam/gradients/zeros_271\n  2.62MiB from batch_normalization_6/FusedBatchNorm\n  2.62MiB from concatenate_5/concat\n  2.62MiB from training_1/Adam/gradients/zeros_277\n  2.37MiB from concatenate_2/concat\n  2.25MiB from concatenate_4/concat\n  2.25MiB from training_1/Adam/gradients/zeros_283\n  1.88MiB from batch_normalization_4/FusedBatchNorm\n  1.88MiB from concatenate_3/concat\n  1.88MiB from training_1/Adam/gradients/zeros_289\n  1.50MiB from batch_normalization_3/FusedBatchNorm\n  1.50MiB from training_1/Adam/gradients/zeros_295\n  1.28MiB from training_1/Adam/gradients/zeros_307\n  1.12MiB from batch_normalization_2/FusedBatchNorm\n  1.12MiB from concatenate_1/concat\n  1.12MiB from training_1/Adam/gradients/zeros_301\n  768.5KiB from batch_normalization_1/FusedBatchNorm\n  768.0KiB from conv2d_1/convolution\n  384.0KiB from training_1/Adam/gradients/zeros_304\n  384.0KiB from dropout_1/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_298\n  384.0KiB from dropout_2/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_292\n  384.0KiB from dropout_3/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_286\n  384.0KiB from dropout_4/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_280\n  384.0KiB from dropout_5/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_274\n  384.0KiB from dropout_6/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_268\n  384.0KiB from dropout_7/cond/dropout/random_uniform/RandomUniform\n  16.5KiB from training_1/Adam/mul_181\n  16.5KiB from training_1/Adam/mul_183\n  15.2KiB from training_1/Adam/mul_571\n  15.2KiB from training_1/Adam/mul_766\n  15.2KiB from training_1/Adam/mul_573\n  15.2KiB from training_1/Adam/mul_768\n  15.2KiB from training_1/Adam/mul_166\n  15.2KiB from training_1/Adam/mul_168\n  15.2KiB from training_1/Adam/mul_376\n  15.2KiB from training_1/Adam/mul_378\n  14.0KiB from training_1/Adam/mul_556\n  14.0KiB from training_1/Adam/mul_751\n  14.0KiB from training_1/Adam/mul_558\n  14.0KiB from training_1/Adam/mul_753\n  14.0KiB from training_1/Adam/mul_151\n  14.0KiB from training_1/Adam/mul_153\n  14.0KiB from training_1/Adam/mul_361\n  14.0KiB from training_1/Adam/mul_363\n  12.8KiB from training_1/Adam/mul_541\n  12.8KiB from training_1/Adam/mul_736\n  12.8KiB from training_1/Adam/mul_543\n  12.8KiB from training_1/Adam/mul_738\n  12.8KiB from training_1/Adam/mul_136\n  12.8KiB from training_1/Adam/mul_138\n  12.8KiB from training_1/Adam/mul_346\n  12.8KiB from training_1/Adam/mul_348\n  12.2KiB from training_1/Adam/mul_781\n  12.2KiB from training_1/Adam/mul_783\n  11.5KiB from training_1/Adam/mul_526\n  11.5KiB from training_1/Adam/mul_721\n  Remaining 291 nodes with 523.8KiB\n\n\t [[Node: loss_1/mul/_3181 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_21641_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  3.62MiB from batch_normalization_5/FusedBatchNorm\n  3.38MiB from concatenate_7/concat\n  3.00MiB from batch_normalization_7/FusedBatchNorm\n  3.00MiB from concatenate_6/concat\n  3.00MiB from training_1/Adam/gradients/zeros_271\n  2.62MiB from batch_normalization_6/FusedBatchNorm\n  2.62MiB from concatenate_5/concat\n  2.62MiB from training_1/Adam/gradients/zeros_277\n  2.37MiB from concatenate_2/concat\n  2.25MiB from concatenate_4/concat\n  2.25MiB from training_1/Adam/gradients/zeros_283\n  1.88MiB from batch_normalization_4/FusedBatchNorm\n  1.88MiB from concatenate_3/concat\n  1.88MiB from training_1/Adam/gradients/zeros_289\n  1.50MiB from batch_normalization_3/FusedBatchNorm\n  1.50MiB from training_1/Adam/gradients/zeros_295\n  1.28MiB from training_1/Adam/gradients/zeros_307\n  1.12MiB from batch_normalization_2/FusedBatchNorm\n  1.12MiB from concatenate_1/concat\n  1.12MiB from training_1/Adam/gradients/zeros_301\n  768.5KiB from batch_normalization_1/FusedBatchNorm\n  768.0KiB from conv2d_1/convolution\n  384.0KiB from training_1/Adam/gradients/zeros_304\n  384.0KiB from dropout_1/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_298\n  384.0KiB from dropout_2/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_292\n  384.0KiB from dropout_3/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_286\n  384.0KiB from dropout_4/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_280\n  384.0KiB from dropout_5/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_274\n  384.0KiB from dropout_6/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_268\n  384.0KiB from dropout_7/cond/dropout/random_uniform/RandomUniform\n  16.5KiB from training_1/Adam/mul_181\n  16.5KiB from training_1/Adam/mul_183\n  15.2KiB from training_1/Adam/mul_571\n  15.2KiB from training_1/Adam/mul_766\n  15.2KiB from training_1/Adam/mul_573\n  15.2KiB from training_1/Adam/mul_768\n  15.2KiB from training_1/Adam/mul_166\n  15.2KiB from training_1/Adam/mul_168\n  15.2KiB from training_1/Adam/mul_376\n  15.2KiB from training_1/Adam/mul_378\n  14.0KiB from training_1/Adam/mul_556\n  14.0KiB from training_1/Adam/mul_751\n  14.0KiB from training_1/Adam/mul_558\n  14.0KiB from training_1/Adam/mul_753\n  14.0KiB from training_1/Adam/mul_151\n  14.0KiB from training_1/Adam/mul_153\n  14.0KiB from training_1/Adam/mul_361\n  14.0KiB from training_1/Adam/mul_363\n  12.8KiB from training_1/Adam/mul_541\n  12.8KiB from training_1/Adam/mul_736\n  12.8KiB from training_1/Adam/mul_543\n  12.8KiB from training_1/Adam/mul_738\n  12.8KiB from training_1/Adam/mul_136\n  12.8KiB from training_1/Adam/mul_138\n  12.8KiB from training_1/Adam/mul_346\n  12.8KiB from training_1/Adam/mul_348\n  12.2KiB from training_1/Adam/mul_781\n  12.2KiB from training_1/Adam/mul_783\n  11.5KiB from training_1/Adam/mul_526\n  11.5KiB from training_1/Adam/mul_721\n  Remaining 291 nodes with 523.8KiB\n\n\nCaused by op u'batch_normalization_8/FusedBatchNorm', defined at:\n  File \"/home/ssk/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/ssk/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 1008, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-2efee5f85923>\", line 7, in <module>\n    First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n  File \"<ipython-input-6-7ad1df438d81>\", line 6, in add_denseblock\n    BatchNorm = BatchNormalization()(temp)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/keras/layers/normalization.py\", line 181, in call\n    epsilon=self.epsilon)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1827, in normalize_batch_in_training\n    epsilon=epsilon)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1802, in _fused_normalize_batch_in_training\n    data_format=tf_data_format)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py\", line 906, in fused_batch_norm\n    name=name)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2224, in _fused_batch_norm\n    is_training=is_training, name=name)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,54,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: batch_normalization_8/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concatenate_7/concat, batch_normalization_8/gamma/read, batch_normalization_8/beta/read, batch_normalization_1/Const_4, batch_normalization_1/Const_4)]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  3.62MiB from batch_normalization_5/FusedBatchNorm\n  3.38MiB from concatenate_7/concat\n  3.00MiB from batch_normalization_7/FusedBatchNorm\n  3.00MiB from concatenate_6/concat\n  3.00MiB from training_1/Adam/gradients/zeros_271\n  2.62MiB from batch_normalization_6/FusedBatchNorm\n  2.62MiB from concatenate_5/concat\n  2.62MiB from training_1/Adam/gradients/zeros_277\n  2.37MiB from concatenate_2/concat\n  2.25MiB from concatenate_4/concat\n  2.25MiB from training_1/Adam/gradients/zeros_283\n  1.88MiB from batch_normalization_4/FusedBatchNorm\n  1.88MiB from concatenate_3/concat\n  1.88MiB from training_1/Adam/gradients/zeros_289\n  1.50MiB from batch_normalization_3/FusedBatchNorm\n  1.50MiB from training_1/Adam/gradients/zeros_295\n  1.28MiB from training_1/Adam/gradients/zeros_307\n  1.12MiB from batch_normalization_2/FusedBatchNorm\n  1.12MiB from concatenate_1/concat\n  1.12MiB from training_1/Adam/gradients/zeros_301\n  768.5KiB from batch_normalization_1/FusedBatchNorm\n  768.0KiB from conv2d_1/convolution\n  384.0KiB from training_1/Adam/gradients/zeros_304\n  384.0KiB from dropout_1/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_298\n  384.0KiB from dropout_2/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_292\n  384.0KiB from dropout_3/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_286\n  384.0KiB from dropout_4/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_280\n  384.0KiB from dropout_5/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_274\n  384.0KiB from dropout_6/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_268\n  384.0KiB from dropout_7/cond/dropout/random_uniform/RandomUniform\n  16.5KiB from training_1/Adam/mul_181\n  16.5KiB from training_1/Adam/mul_183\n  15.2KiB from training_1/Adam/mul_571\n  15.2KiB from training_1/Adam/mul_766\n  15.2KiB from training_1/Adam/mul_573\n  15.2KiB from training_1/Adam/mul_768\n  15.2KiB from training_1/Adam/mul_166\n  15.2KiB from training_1/Adam/mul_168\n  15.2KiB from training_1/Adam/mul_376\n  15.2KiB from training_1/Adam/mul_378\n  14.0KiB from training_1/Adam/mul_556\n  14.0KiB from training_1/Adam/mul_751\n  14.0KiB from training_1/Adam/mul_558\n  14.0KiB from training_1/Adam/mul_753\n  14.0KiB from training_1/Adam/mul_151\n  14.0KiB from training_1/Adam/mul_153\n  14.0KiB from training_1/Adam/mul_361\n  14.0KiB from training_1/Adam/mul_363\n  12.8KiB from training_1/Adam/mul_541\n  12.8KiB from training_1/Adam/mul_736\n  12.8KiB from training_1/Adam/mul_543\n  12.8KiB from training_1/Adam/mul_738\n  12.8KiB from training_1/Adam/mul_136\n  12.8KiB from training_1/Adam/mul_138\n  12.8KiB from training_1/Adam/mul_346\n  12.8KiB from training_1/Adam/mul_348\n  12.2KiB from training_1/Adam/mul_781\n  12.2KiB from training_1/Adam/mul_783\n  11.5KiB from training_1/Adam/mul_526\n  11.5KiB from training_1/Adam/mul_721\n  Remaining 291 nodes with 523.8KiB\n\n\t [[Node: loss_1/mul/_3181 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_21641_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  3.62MiB from batch_normalization_5/FusedBatchNorm\n  3.38MiB from concatenate_7/concat\n  3.00MiB from batch_normalization_7/FusedBatchNorm\n  3.00MiB from concatenate_6/concat\n  3.00MiB from training_1/Adam/gradients/zeros_271\n  2.62MiB from batch_normalization_6/FusedBatchNorm\n  2.62MiB from concatenate_5/concat\n  2.62MiB from training_1/Adam/gradients/zeros_277\n  2.37MiB from concatenate_2/concat\n  2.25MiB from concatenate_4/concat\n  2.25MiB from training_1/Adam/gradients/zeros_283\n  1.88MiB from batch_normalization_4/FusedBatchNorm\n  1.88MiB from concatenate_3/concat\n  1.88MiB from training_1/Adam/gradients/zeros_289\n  1.50MiB from batch_normalization_3/FusedBatchNorm\n  1.50MiB from training_1/Adam/gradients/zeros_295\n  1.28MiB from training_1/Adam/gradients/zeros_307\n  1.12MiB from batch_normalization_2/FusedBatchNorm\n  1.12MiB from concatenate_1/concat\n  1.12MiB from training_1/Adam/gradients/zeros_301\n  768.5KiB from batch_normalization_1/FusedBatchNorm\n  768.0KiB from conv2d_1/convolution\n  384.0KiB from training_1/Adam/gradients/zeros_304\n  384.0KiB from dropout_1/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_298\n  384.0KiB from dropout_2/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_292\n  384.0KiB from dropout_3/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_286\n  384.0KiB from dropout_4/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_280\n  384.0KiB from dropout_5/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_274\n  384.0KiB from dropout_6/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_268\n  384.0KiB from dropout_7/cond/dropout/random_uniform/RandomUniform\n  16.5KiB from training_1/Adam/mul_181\n  16.5KiB from training_1/Adam/mul_183\n  15.2KiB from training_1/Adam/mul_571\n  15.2KiB from training_1/Adam/mul_766\n  15.2KiB from training_1/Adam/mul_573\n  15.2KiB from training_1/Adam/mul_768\n  15.2KiB from training_1/Adam/mul_166\n  15.2KiB from training_1/Adam/mul_168\n  15.2KiB from training_1/Adam/mul_376\n  15.2KiB from training_1/Adam/mul_378\n  14.0KiB from training_1/Adam/mul_556\n  14.0KiB from training_1/Adam/mul_751\n  14.0KiB from training_1/Adam/mul_558\n  14.0KiB from training_1/Adam/mul_753\n  14.0KiB from training_1/Adam/mul_151\n  14.0KiB from training_1/Adam/mul_153\n  14.0KiB from training_1/Adam/mul_361\n  14.0KiB from training_1/Adam/mul_363\n  12.8KiB from training_1/Adam/mul_541\n  12.8KiB from training_1/Adam/mul_736\n  12.8KiB from training_1/Adam/mul_543\n  12.8KiB from training_1/Adam/mul_738\n  12.8KiB from training_1/Adam/mul_136\n  12.8KiB from training_1/Adam/mul_138\n  12.8KiB from training_1/Adam/mul_346\n  12.8KiB from training_1/Adam/mul_348\n  12.2KiB from training_1/Adam/mul_781\n  12.2KiB from training_1/Adam/mul_783\n  11.5KiB from training_1/Adam/mul_526\n  11.5KiB from training_1/Adam/mul_721\n  Remaining 291 nodes with 523.8KiB\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1ed392c6bd73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m/home/ssk/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/ssk/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ssk/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[16,54,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: batch_normalization_8/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concatenate_7/concat, batch_normalization_8/gamma/read, batch_normalization_8/beta/read, batch_normalization_1/Const_4, batch_normalization_1/Const_4)]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  3.62MiB from batch_normalization_5/FusedBatchNorm\n  3.38MiB from concatenate_7/concat\n  3.00MiB from batch_normalization_7/FusedBatchNorm\n  3.00MiB from concatenate_6/concat\n  3.00MiB from training_1/Adam/gradients/zeros_271\n  2.62MiB from batch_normalization_6/FusedBatchNorm\n  2.62MiB from concatenate_5/concat\n  2.62MiB from training_1/Adam/gradients/zeros_277\n  2.37MiB from concatenate_2/concat\n  2.25MiB from concatenate_4/concat\n  2.25MiB from training_1/Adam/gradients/zeros_283\n  1.88MiB from batch_normalization_4/FusedBatchNorm\n  1.88MiB from concatenate_3/concat\n  1.88MiB from training_1/Adam/gradients/zeros_289\n  1.50MiB from batch_normalization_3/FusedBatchNorm\n  1.50MiB from training_1/Adam/gradients/zeros_295\n  1.28MiB from training_1/Adam/gradients/zeros_307\n  1.12MiB from batch_normalization_2/FusedBatchNorm\n  1.12MiB from concatenate_1/concat\n  1.12MiB from training_1/Adam/gradients/zeros_301\n  768.5KiB from batch_normalization_1/FusedBatchNorm\n  768.0KiB from conv2d_1/convolution\n  384.0KiB from training_1/Adam/gradients/zeros_304\n  384.0KiB from dropout_1/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_298\n  384.0KiB from dropout_2/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_292\n  384.0KiB from dropout_3/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_286\n  384.0KiB from dropout_4/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_280\n  384.0KiB from dropout_5/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_274\n  384.0KiB from dropout_6/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_268\n  384.0KiB from dropout_7/cond/dropout/random_uniform/RandomUniform\n  16.5KiB from training_1/Adam/mul_181\n  16.5KiB from training_1/Adam/mul_183\n  15.2KiB from training_1/Adam/mul_571\n  15.2KiB from training_1/Adam/mul_766\n  15.2KiB from training_1/Adam/mul_573\n  15.2KiB from training_1/Adam/mul_768\n  15.2KiB from training_1/Adam/mul_166\n  15.2KiB from training_1/Adam/mul_168\n  15.2KiB from training_1/Adam/mul_376\n  15.2KiB from training_1/Adam/mul_378\n  14.0KiB from training_1/Adam/mul_556\n  14.0KiB from training_1/Adam/mul_751\n  14.0KiB from training_1/Adam/mul_558\n  14.0KiB from training_1/Adam/mul_753\n  14.0KiB from training_1/Adam/mul_151\n  14.0KiB from training_1/Adam/mul_153\n  14.0KiB from training_1/Adam/mul_361\n  14.0KiB from training_1/Adam/mul_363\n  12.8KiB from training_1/Adam/mul_541\n  12.8KiB from training_1/Adam/mul_736\n  12.8KiB from training_1/Adam/mul_543\n  12.8KiB from training_1/Adam/mul_738\n  12.8KiB from training_1/Adam/mul_136\n  12.8KiB from training_1/Adam/mul_138\n  12.8KiB from training_1/Adam/mul_346\n  12.8KiB from training_1/Adam/mul_348\n  12.2KiB from training_1/Adam/mul_781\n  12.2KiB from training_1/Adam/mul_783\n  11.5KiB from training_1/Adam/mul_526\n  11.5KiB from training_1/Adam/mul_721\n  Remaining 291 nodes with 523.8KiB\n\n\t [[Node: loss_1/mul/_3181 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_21641_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  3.62MiB from batch_normalization_5/FusedBatchNorm\n  3.38MiB from concatenate_7/concat\n  3.00MiB from batch_normalization_7/FusedBatchNorm\n  3.00MiB from concatenate_6/concat\n  3.00MiB from training_1/Adam/gradients/zeros_271\n  2.62MiB from batch_normalization_6/FusedBatchNorm\n  2.62MiB from concatenate_5/concat\n  2.62MiB from training_1/Adam/gradients/zeros_277\n  2.37MiB from concatenate_2/concat\n  2.25MiB from concatenate_4/concat\n  2.25MiB from training_1/Adam/gradients/zeros_283\n  1.88MiB from batch_normalization_4/FusedBatchNorm\n  1.88MiB from concatenate_3/concat\n  1.88MiB from training_1/Adam/gradients/zeros_289\n  1.50MiB from batch_normalization_3/FusedBatchNorm\n  1.50MiB from training_1/Adam/gradients/zeros_295\n  1.28MiB from training_1/Adam/gradients/zeros_307\n  1.12MiB from batch_normalization_2/FusedBatchNorm\n  1.12MiB from concatenate_1/concat\n  1.12MiB from training_1/Adam/gradients/zeros_301\n  768.5KiB from batch_normalization_1/FusedBatchNorm\n  768.0KiB from conv2d_1/convolution\n  384.0KiB from training_1/Adam/gradients/zeros_304\n  384.0KiB from dropout_1/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_298\n  384.0KiB from dropout_2/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_292\n  384.0KiB from dropout_3/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_286\n  384.0KiB from dropout_4/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_280\n  384.0KiB from dropout_5/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_274\n  384.0KiB from dropout_6/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_268\n  384.0KiB from dropout_7/cond/dropout/random_uniform/RandomUniform\n  16.5KiB from training_1/Adam/mul_181\n  16.5KiB from training_1/Adam/mul_183\n  15.2KiB from training_1/Adam/mul_571\n  15.2KiB from training_1/Adam/mul_766\n  15.2KiB from training_1/Adam/mul_573\n  15.2KiB from training_1/Adam/mul_768\n  15.2KiB from training_1/Adam/mul_166\n  15.2KiB from training_1/Adam/mul_168\n  15.2KiB from training_1/Adam/mul_376\n  15.2KiB from training_1/Adam/mul_378\n  14.0KiB from training_1/Adam/mul_556\n  14.0KiB from training_1/Adam/mul_751\n  14.0KiB from training_1/Adam/mul_558\n  14.0KiB from training_1/Adam/mul_753\n  14.0KiB from training_1/Adam/mul_151\n  14.0KiB from training_1/Adam/mul_153\n  14.0KiB from training_1/Adam/mul_361\n  14.0KiB from training_1/Adam/mul_363\n  12.8KiB from training_1/Adam/mul_541\n  12.8KiB from training_1/Adam/mul_736\n  12.8KiB from training_1/Adam/mul_543\n  12.8KiB from training_1/Adam/mul_738\n  12.8KiB from training_1/Adam/mul_136\n  12.8KiB from training_1/Adam/mul_138\n  12.8KiB from training_1/Adam/mul_346\n  12.8KiB from training_1/Adam/mul_348\n  12.2KiB from training_1/Adam/mul_781\n  12.2KiB from training_1/Adam/mul_783\n  11.5KiB from training_1/Adam/mul_526\n  11.5KiB from training_1/Adam/mul_721\n  Remaining 291 nodes with 523.8KiB\n\n\nCaused by op u'batch_normalization_8/FusedBatchNorm', defined at:\n  File \"/home/ssk/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/ssk/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 1008, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-2efee5f85923>\", line 7, in <module>\n    First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n  File \"<ipython-input-6-7ad1df438d81>\", line 6, in add_denseblock\n    BatchNorm = BatchNormalization()(temp)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/keras/layers/normalization.py\", line 181, in call\n    epsilon=self.epsilon)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1827, in normalize_batch_in_training\n    epsilon=epsilon)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1802, in _fused_normalize_batch_in_training\n    data_format=tf_data_format)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py\", line 906, in fused_batch_norm\n    name=name)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2224, in _fused_batch_norm\n    is_training=is_training, name=name)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/home/ssk/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,54,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: batch_normalization_8/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concatenate_7/concat, batch_normalization_8/gamma/read, batch_normalization_8/beta/read, batch_normalization_1/Const_4, batch_normalization_1/Const_4)]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  3.62MiB from batch_normalization_5/FusedBatchNorm\n  3.38MiB from concatenate_7/concat\n  3.00MiB from batch_normalization_7/FusedBatchNorm\n  3.00MiB from concatenate_6/concat\n  3.00MiB from training_1/Adam/gradients/zeros_271\n  2.62MiB from batch_normalization_6/FusedBatchNorm\n  2.62MiB from concatenate_5/concat\n  2.62MiB from training_1/Adam/gradients/zeros_277\n  2.37MiB from concatenate_2/concat\n  2.25MiB from concatenate_4/concat\n  2.25MiB from training_1/Adam/gradients/zeros_283\n  1.88MiB from batch_normalization_4/FusedBatchNorm\n  1.88MiB from concatenate_3/concat\n  1.88MiB from training_1/Adam/gradients/zeros_289\n  1.50MiB from batch_normalization_3/FusedBatchNorm\n  1.50MiB from training_1/Adam/gradients/zeros_295\n  1.28MiB from training_1/Adam/gradients/zeros_307\n  1.12MiB from batch_normalization_2/FusedBatchNorm\n  1.12MiB from concatenate_1/concat\n  1.12MiB from training_1/Adam/gradients/zeros_301\n  768.5KiB from batch_normalization_1/FusedBatchNorm\n  768.0KiB from conv2d_1/convolution\n  384.0KiB from training_1/Adam/gradients/zeros_304\n  384.0KiB from dropout_1/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_298\n  384.0KiB from dropout_2/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_292\n  384.0KiB from dropout_3/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_286\n  384.0KiB from dropout_4/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_280\n  384.0KiB from dropout_5/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_274\n  384.0KiB from dropout_6/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_268\n  384.0KiB from dropout_7/cond/dropout/random_uniform/RandomUniform\n  16.5KiB from training_1/Adam/mul_181\n  16.5KiB from training_1/Adam/mul_183\n  15.2KiB from training_1/Adam/mul_571\n  15.2KiB from training_1/Adam/mul_766\n  15.2KiB from training_1/Adam/mul_573\n  15.2KiB from training_1/Adam/mul_768\n  15.2KiB from training_1/Adam/mul_166\n  15.2KiB from training_1/Adam/mul_168\n  15.2KiB from training_1/Adam/mul_376\n  15.2KiB from training_1/Adam/mul_378\n  14.0KiB from training_1/Adam/mul_556\n  14.0KiB from training_1/Adam/mul_751\n  14.0KiB from training_1/Adam/mul_558\n  14.0KiB from training_1/Adam/mul_753\n  14.0KiB from training_1/Adam/mul_151\n  14.0KiB from training_1/Adam/mul_153\n  14.0KiB from training_1/Adam/mul_361\n  14.0KiB from training_1/Adam/mul_363\n  12.8KiB from training_1/Adam/mul_541\n  12.8KiB from training_1/Adam/mul_736\n  12.8KiB from training_1/Adam/mul_543\n  12.8KiB from training_1/Adam/mul_738\n  12.8KiB from training_1/Adam/mul_136\n  12.8KiB from training_1/Adam/mul_138\n  12.8KiB from training_1/Adam/mul_346\n  12.8KiB from training_1/Adam/mul_348\n  12.2KiB from training_1/Adam/mul_781\n  12.2KiB from training_1/Adam/mul_783\n  11.5KiB from training_1/Adam/mul_526\n  11.5KiB from training_1/Adam/mul_721\n  Remaining 291 nodes with 523.8KiB\n\n\t [[Node: loss_1/mul/_3181 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_21641_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  3.62MiB from batch_normalization_5/FusedBatchNorm\n  3.38MiB from concatenate_7/concat\n  3.00MiB from batch_normalization_7/FusedBatchNorm\n  3.00MiB from concatenate_6/concat\n  3.00MiB from training_1/Adam/gradients/zeros_271\n  2.62MiB from batch_normalization_6/FusedBatchNorm\n  2.62MiB from concatenate_5/concat\n  2.62MiB from training_1/Adam/gradients/zeros_277\n  2.37MiB from concatenate_2/concat\n  2.25MiB from concatenate_4/concat\n  2.25MiB from training_1/Adam/gradients/zeros_283\n  1.88MiB from batch_normalization_4/FusedBatchNorm\n  1.88MiB from concatenate_3/concat\n  1.88MiB from training_1/Adam/gradients/zeros_289\n  1.50MiB from batch_normalization_3/FusedBatchNorm\n  1.50MiB from training_1/Adam/gradients/zeros_295\n  1.28MiB from training_1/Adam/gradients/zeros_307\n  1.12MiB from batch_normalization_2/FusedBatchNorm\n  1.12MiB from concatenate_1/concat\n  1.12MiB from training_1/Adam/gradients/zeros_301\n  768.5KiB from batch_normalization_1/FusedBatchNorm\n  768.0KiB from conv2d_1/convolution\n  384.0KiB from training_1/Adam/gradients/zeros_304\n  384.0KiB from dropout_1/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_298\n  384.0KiB from dropout_2/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_292\n  384.0KiB from dropout_3/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_286\n  384.0KiB from dropout_4/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_280\n  384.0KiB from dropout_5/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_274\n  384.0KiB from dropout_6/cond/dropout/random_uniform/RandomUniform\n  384.0KiB from training_1/Adam/gradients/zeros_268\n  384.0KiB from dropout_7/cond/dropout/random_uniform/RandomUniform\n  16.5KiB from training_1/Adam/mul_181\n  16.5KiB from training_1/Adam/mul_183\n  15.2KiB from training_1/Adam/mul_571\n  15.2KiB from training_1/Adam/mul_766\n  15.2KiB from training_1/Adam/mul_573\n  15.2KiB from training_1/Adam/mul_768\n  15.2KiB from training_1/Adam/mul_166\n  15.2KiB from training_1/Adam/mul_168\n  15.2KiB from training_1/Adam/mul_376\n  15.2KiB from training_1/Adam/mul_378\n  14.0KiB from training_1/Adam/mul_556\n  14.0KiB from training_1/Adam/mul_751\n  14.0KiB from training_1/Adam/mul_558\n  14.0KiB from training_1/Adam/mul_753\n  14.0KiB from training_1/Adam/mul_151\n  14.0KiB from training_1/Adam/mul_153\n  14.0KiB from training_1/Adam/mul_361\n  14.0KiB from training_1/Adam/mul_363\n  12.8KiB from training_1/Adam/mul_541\n  12.8KiB from training_1/Adam/mul_736\n  12.8KiB from training_1/Adam/mul_543\n  12.8KiB from training_1/Adam/mul_738\n  12.8KiB from training_1/Adam/mul_136\n  12.8KiB from training_1/Adam/mul_138\n  12.8KiB from training_1/Adam/mul_346\n  12.8KiB from training_1/Adam/mul_348\n  12.2KiB from training_1/Adam/mul_781\n  12.2KiB from training_1/Adam/mul_783\n  11.5KiB from training_1/Adam/mul_526\n  11.5KiB from training_1/Adam/mul_721\n  Remaining 291 nodes with 523.8KiB\n\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14029,
     "status": "ok",
     "timestamp": 1519133071108,
     "user": {
      "displayName": "Rohan Shravan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116259693572096743881"
     },
     "user_tz": -330
    },
    "id": "ZcWydmIVhZGr",
    "outputId": "a0345aa5-79ff-4e56-eb94-50437b43c4fe"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9796,
     "status": "ok",
     "timestamp": 1519133080926,
     "user": {
      "displayName": "Rohan Shravan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116259693572096743881"
     },
     "user_tz": -330
    },
    "id": "UE3lF6EH1r_L",
    "outputId": "92df862c-76a7-4a02-9533-6c164bc5984d"
   },
   "outputs": [],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ai-yZ2ED5AK1"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('DNST_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Og56VCRh5j8V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "DNST_CIFAR10_AUG.ipynb",
   "provenance": [
    {
     "file_id": "1_1kwmwgL7g94jI6BEtcgm-D2_AFk0zxK",
     "timestamp": 1519101209834
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
